{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm_name = \"gpt-4-1106-preview\"\n",
    "os.environ[\"OPENAI_API_KEY\"]  # you should see the api key if the .env file is loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "persist_directory = os.environ.get(\"PERSIST_DIRECTORY\", \"/tmp\")\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "vectordb = Chroma(\n",
    "    collection_name=\"hummingbot_documentation\",\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context that are part of the Hummingbot Documentation to answer the user question. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "model = ChatOpenAI(model_name=llm_name, temperature=0)\n",
    "output_parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "query = \"How to use the pure market making strategy?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableMap\n",
    "\n",
    "retriever = vectordb.as_retriever(search_type=\"mmr\", k=5, fetch_k=10)\n",
    "\n",
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "chain.invoke(\n",
    "    input={\n",
    "        \"question\": query,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
